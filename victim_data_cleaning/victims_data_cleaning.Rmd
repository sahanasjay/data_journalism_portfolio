---
title: "Newspaper Lynching Coverage"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Load libraries, API keys, environment variables
```{r}

# Load libraries
library(janitor)
library(tidyverse)
library(lubridate)
library(httr)
library(jsonlite)
library(gargle)
library(googlesheets4)
library(fs)
library(tigris)
library(qpdf)
library(readtext)
library(parallel)
library(purrr)
library(tidyjson)
library(readxl)
library(XML)
library(fuzzyjoin)

```


## Load and Clean data

### State and County Standarization
Use fips code library for state and county standardization.
```{r}
# Load fips_codes
fips <- fips_codes

# Create a dataframe called "state_list" with just the state names and abbreviations by selecting those columns from fips dataframe
state_list <- fips %>%
  select(state, state_name) %>%
  distinct(state, state_name)

# Create a version of the state list where the names (but not abbreviations) are lowercase
state_list_lower <- state_list %>%
  mutate(state_name = tolower(state_name))

```

### Create list of current newspapers
Preparing to scraping newspaper lineage information from the Library of Congress newspaper directory, we needed a list of current newspapers and their urls. Current newspapers are considered "level 0" in the lineage. The scraper we built in Python visits the pages for the current newspapers and grabs information about previous titles as "level 1." It then visits the level 1 pages to build level 2, and so on.

This current newspapers list is derived from the newspaper directory API information that Sean Mussenden cleaned. [No documentation on Sean's cleaning has yet been done.]
```{r}

# Create a dataframe of all current newspapers by filtering the "updated_all_loc_newspapers_cleaned" dataframe on the end year column
current_newspapers <- readRDS("output_data/loc_newspapers_list/updated_all_loc_newspapers_cleaned.rds") %>%
  filter(updated_end_year == "current")

# Replace ".json" with "/" at the end of the urls
current_newspapers$url <- str_replace(current_newspapers$url, ".json", "/")

# Select only columns needed for lineage dataframe
current_newspapers_with_level <- current_newspapers %>%
  select(title, url, lccn) %>%
  #Add current_lccn column and level column set at "0"
  mutate(current_lccn = lccn, level = "0")

# Make sure the level number is numeric 
current_newspapers_with_level$level <- as.numeric(current_newspapers_with_level$level)


```


### Read in data from Library of Congress newspaper directory scraper
Before scraping the Library of Congress newspaper directory we divided the current newspapers list into several parts to make the scraper run more smoothly. That meant that the scraper yielded multiple csv files for each lineage level. Here, we took all of those files and combine them into one data frame to analyze.
```{r}
# Set directory to the folder containing csv files generated by scraper. List files in location.
scraper_results_loc = list.files(path = "../data/handmade/loc_newspaper_directory_webscraper_results", pattern="*.csv", full.names=TRUE)


# Read in all csv files from set location as an object called "newspaper_lineage."
newspaper_lineage = lapply(scraper_results_loc, read_csv) %>%
  bind_rows()


# Bind current newspapers to newspaper_lineage dataframe
full_newspaper_lineage <- bind_rows(newspaper_lineage, current_newspapers_with_level)

```

### Join our lineage dataset to the all_loc_newspapers dataset
We joined full_newspaper_lineage with the master list of newspapers by the historical title's lccn. This allowed us to add back in information about the historical titles' location and years of publication.
```{r}

# Inner join on lccn

# Read in all_loc_newspapers dataset
all_loc_newspapers <- readRDS("output_data/loc_newspapers_list/updated_all_loc_newspapers_cleaned.rds") %>%
  select(-title, -title_normal, -url, -id)

# Create a new dataframe called "joined_loc_newspapers" by inner joining all_loc_newspapers with newspaper_lineage. Join on the "lccn" value.
joined_loc_newspapers <- all_loc_newspapers %>%
  inner_join(full_newspaper_lineage, by = "lccn")


# Do an anti-join to identify which newspapers from the lineage dataset did not have a match in the all_loc_newspapers data. There are 379 newspapers in this category. Some of these are foreign language newspapers so they were excluded from all_loc but there are others for which we haven't determined the cause of omission.
missing_newspapers <- newspaper_lineage %>%
  anti_join(all_loc_newspapers, by = "lccn")

```


### Read in victim data

Data documentation: 
**date_type** = created to help viewers understand values in day_clean. Three options: "no_day" means there was no day value provided for a given lynching. "inexact_day" means the day was provided and had a question mark next to it i.e. 23?) or a date range was provided. "exact_day" if an exact day was provided in the original dataset. All zeroes and values with question marks converted to ones in "day_clean," the earlier of two dates in a date range selected.

```{r}
# Clean Tolnay-Beck data on lynching victims from states with most lynchings, filtering to return for Black victims only
lynching_victims_new <-read_xlsx("../data/source/lynching_victims/tolnay_beck_victim_5_july_2019.xlsx") %>%
  clean_names() %>%
  mutate(status = tolower(status)) %>%
  mutate(status = case_when(status == "probable lynching" ~  "probable victim",
                            status == "probably lynching" ~ "probable victim",
                            status == "possible lynching" ~ "possible victim",
                            status == "probable lynching - suicide" ~ "probable victim - suicide",
                            status == "possible lynching - suicide" ~ "possible victim - suicide",
                            status == "possibly lynching" ~ "possible victim",
                            status == "possibly lynching - suicide" ~ "possible victim - suicide",
                            status == "probably lynching - suicide" ~ "probable victim - suicide",
                            status == "lynching" ~ "victim",
                            TRUE ~ status)) %>%
  mutate(data_source = "victim tolnay beck") %>%
  mutate(victims_black = str_extract(victims_race, "Black")) %>%
  filter(victims_black != is.na(victims_black)) %>%
  mutate(day_clean = day) %>%
  filter(month != "0") %>%
  mutate(date_type = ifelse(day == "?"|day == "0", "no_day", "exact_day")) %>%
  mutate(day_clean = case_when(day_clean == "0" ~ "1",
                               day_clean == "?" ~ "1",
                               TRUE ~ day_clean)) %>%
  mutate(date = make_date(year, month, day_clean)) %>%
  mutate(city = "unknown")

# Create new dataframe with 13 columns instead of 29. Include gender and age for separate analysis.
lynching_victims_trimmed_with_gender<- lynching_victims_new%>%
  mutate(lynch_county = tolower(lynch_county))%>%
  mutate(gender = victims_sex) %>% 
  select(name, date, city, lynch_county, lynch_state, status, gender, victims_age, data_source, date_type, day, day_clean, month, year)

# Remove gender and age to prep for combining with threats data
lynching_victims_trimmed <- lynching_victims_trimmed_with_gender %>% 
  select(name, date, city, lynch_county, lynch_state, status, data_source, date_type, day, day_clean, month, year)

# Clean Tolnay-Beck data on lynching threats from 13 states, filtering to return for potential victims who were Black
lynching_threats <- read_xlsx("../data/source/lynching_victims/tolnay_beck_threats_5_july_2019.xlsx") %>%
  row_to_names(row_number = 1) %>%
  clean_names() %>%
  mutate(data_source = "threat tolnay beck") %>%
  rename(potential_victim_name_race_sex = potential_victim_s_name_s_race_sex) %>%
  mutate(victims_black = str_extract(potential_victim_name_race_sex, "Black")) %>%
  filter(victims_black != is.na(victims_black)) %>%
  mutate(day_clean = day) %>%
  mutate(day_clean = case_when(day_clean == "42287" ~ "1",
                         TRUE ~ day_clean)) %>%
  mutate(date_type = ifelse(str_detect(day, "\\d+\\-\\d+"), "inexact_day", ifelse(str_detect(day, "\\?")|str_detect(day, "-")|day == "42287", "no_day", "exact_day")))%>%
  mutate(day_clean = case_when(str_detect(day_clean, "\\?") ~ str_replace(day_clean,"^(.*?)\\?", "1"),
         TRUE ~ day_clean)) %>%
  mutate(day_clean = case_when(str_detect(day_clean, "\\d+\\-\\d+") ~ substr(day_clean,1,2),
         TRUE ~ day_clean)) %>%
  mutate(day_clean = case_when(str_detect(day_clean, "-") ~ str_replace(day_clean,"-", "1"),
         TRUE ~ day_clean)) %>%
  filter(month != "-") %>%
  mutate(date = make_date(year, month, day_clean))%>%
  mutate(city = "unknown")

# Create new dataframe with 12 columns instead of many more
lynching_threats_trimmed<- lynching_threats %>%
  rename(name = potential_victim_name_race_sex) %>%
  rename(lynch_county = lynch_co) %>%
  mutate(lynch_county = tolower(lynch_county)) %>%
  mutate(month = as.numeric(month)) %>%
  mutate(year = as.numeric(year)) %>%
  select(name, date, city, lynch_county, lynch_state, status, data_source, date_type, day, day_clean, month, year)

# Clean Seguin data on lynching victims from additional states, filtering to return for Black victims only
more_victims_seguin <- read.csv("../data/source/lynching_victims/seguin_data.csv") %>%
  mutate(status = "victim") %>%
  mutate(data_source = "victim seguin") %>%
  mutate(victims_black = str_extract(race, "Black")) %>%
  filter(victims_black != is.na(victims_black)) %>%
  filter(month != "0") %>%
  mutate(day_clean = day) %>%
  mutate(date_type = ifelse(day == "0", "no_day", "exact_day")) %>%
  mutate(day_clean = as.character(day_clean)) %>%
  mutate(day_clean = case_when(day_clean == "0" ~ "1",
                               TRUE ~ day_clean)) %>%
  mutate(date = make_date(year, month, day_clean)) %>%
  mutate(city = case_when(city == "." ~ "unknown",
                          TRUE ~ city))

# Create new dataframe with 13 columns instead of many more. Include gender for separate analysis.
seguin_victims_trimmed_with_gender<- more_victims_seguin %>%
  rename(name = victim) %>%
  rename(lynch_county = county) %>%
  rename(lynch_state = state) %>%
  mutate(lynch_county = tolower(lynch_county)) %>%
  mutate(city = tolower(city)) %>%
  mutate(day = as.character(day)) %>%
  select(name, date, city, lynch_county, lynch_state, status, gender, data_source, date_type, day, day_clean, month, year)

# Remove gender to prep for combining with threats data. Now down to 12 columns
seguin_victims_trimmed <- seguin_victims_trimmed_with_gender %>%
  select(name, date, city, lynch_county, lynch_state, status, data_source, date_type, day, day_clean, month, year)


# Combine all three trimmed dataframes into a single dataset
black_victims_threats <- lynching_victims_trimmed %>%
  bind_rows(lynching_threats_trimmed) %>%
  bind_rows(seguin_victims_trimmed) %>%
  rename(state = lynch_state) %>%
  rename(county = lynch_county) %>%
  mutate(status = tolower(status))

# Combine Tolnay-Beck and Seguin data on lynchings for gender analysis

black_victims_with_gender <- lynching_victims_trimmed_with_gender %>% 
  select(-victims_age) %>% 
  bind_rows(seguin_victims_trimmed_with_gender)


#write_csv(black_victims_threats, "../data/processed/etl_black_victims_threats.csv")
#write_rds(black_victims_threats, "../data/processed/etl_black_victims_threats.rds")

write_csv(black_victims_with_gender, "../data/processed/etl_black_victims_with_gender.csv")
write_rds(black_victims_with_gender, "../data/processed/etl_black_victims_with_gender.rds")


```