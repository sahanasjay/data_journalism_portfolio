{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e629f1-9e21-43e0-9e23-2af0c0b247e2",
   "metadata": {},
   "source": [
    "# Library of Congress Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a21912-ee49-4d4e-a4ff-86e3c6820f9d",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0818376b-9b90-44a4-b818-3e6e813bcbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas for general data wrangling\n",
    "import pandas as pd\n",
    "\n",
    "#selenium for robot browsing\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96569c4-415c-4033-b05d-7ca337348617",
   "metadata": {},
   "source": [
    "## Define Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40bb299f-dedb-4d39-852e-e5beda04a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def family_tree():\n",
    "    \n",
    "    # Set heroku webdriver options\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "   \n",
    "    # Set window size\n",
    "    chrome_options.add_argument(\"window-size=1400,800\")\n",
    "    # chrome_options.add_argument(\"--headless\")\n",
    "    \n",
    "    ####### CSV NAME #######\n",
    "    # Read in csv of current U.S. newspapers as dataframe\n",
    "    level_0 = pd.read_csv(\"current_newspapers_c.csv\")\n",
    "    \n",
    "    \n",
    "    # Retrieve only the columns needed\n",
    "    df_loop_through = level_0[[\"title\", \"url\", \"lccn\"]]\n",
    "    \n",
    "    # Create a column for lccn of newspapers that currently exist\n",
    "    df_loop_through['current_lccn'] = df_loop_through['lccn']\n",
    "    \n",
    "    # Create a column for level of newspaper lineage. Level 1 is a newspaper immediately preceding a currently operating newspaper, level 2 is a newspaper immediately preceding a Level 1 newspaper, and so on.\n",
    "    df_loop_through['level'] = \"1\"\n",
    "    \n",
    "    # Define the path to the robot browser (tell it where to find the driver, rename to driver)\n",
    "    driver = webdriver.Chrome(executable_path = \"chromedriver\", options = chrome_options)\n",
    "    \n",
    "    # Create an empty list for checking duplicates\n",
    "    dupes_check = []\n",
    "    \n",
    "    # Write a for loop that will increase the lineage level by 1 every time the scraper runs through the loops nested below until it has run 20 times\n",
    "    for lineage_level in range(0,30):\n",
    "        lineage_value = lineage_level+1\n",
    "        \n",
    "        # Create an empty dataframe with column names that match the dataframe that contains current newspapers \n",
    "        temp_df = pd.DataFrame(columns=[\"title\", \"url\", \"lccn\", \"current_lccn\", \"level\"])\n",
    "        \n",
    "        # Write a for loop that will iterate through each row in the selected dataframe\n",
    "        for row in df_loop_through.itertuples():\n",
    "           \n",
    "            # Identify that in every row, the second value will be a url\n",
    "            url = row[2]\n",
    "           \n",
    "            # Identify that in every row, the fourth value will be the current lccn\n",
    "            current_lccn = row[4]\n",
    "            \n",
    "            # Tell the robot browser to visit every url\n",
    "            driver.get(url)\n",
    "           \n",
    "            # Direct scraper to the page element that follows the \"Preceding Titles:\" header\n",
    "            preceding_titles = driver.find_elements_by_xpath(\"//dt[text()='Preceding Titles:']/following::dd[1]/ul/li/a\")\n",
    "            \n",
    "            # Wait for \"Preceding Titles\" to load\n",
    "            # preceding_titles = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.XPATH, \"//dt[text()='Preceding Titles:']/following::dd[1]/ul/li/a\"))) \n",
    "            \n",
    "            # Write a for loop that gathers information about each link under the \"Preceding Titles:\" header\n",
    "            for link in preceding_titles:\n",
    "                # Grab the text of each link (these are the actual titles)\n",
    "                link_text = link.text\n",
    "               \n",
    "                # Grab each url\n",
    "                link_url = link.get_attribute(\"href\")\n",
    "                \n",
    "                # Grab the lccn by extracting the characters between the penultimate slash and final slash in the url\n",
    "                link_lccn = link.get_attribute(\"href\").rstrip('/').split('/')[-1]\n",
    "                \n",
    "                ##### Check for duplicates #####\n",
    "                # Add link_url to duplicates check list\n",
    "                dupes_check.append(link_url)\n",
    "               \n",
    "                # Convert duplicates check list to a set. Sets automatically remove duplicates.\n",
    "                #dupes_set = set(dupes_check)\n",
    "               \n",
    "                # Check if the list and set lengths are equal. If they are equal, there are no duplicates. If they are not equal, the list has duplicates.\n",
    "                if len(dupes_check) != len(set(dupes_check)):\n",
    "                    dupes_check = list(set(dupes_check))\n",
    "                   \n",
    "                else:  \n",
    "                    # Create a list from these elements\n",
    "                    preceding_titles_list = [link_text,link_url,link_lccn,current_lccn,lineage_value]\n",
    "                    \n",
    "                    # Index list items to the columns that were created in the empty dataframe\n",
    "                    preceding_titles_series = pd.Series(preceding_titles_list,index=temp_df.columns)\n",
    "                    \n",
    "                    # Append series to empty dataframe. Information for each preceding title will appear as a single row.\n",
    "                    temp_df = temp_df.append(preceding_titles_series,ignore_index=True) \n",
    "                    dupes_check = list(set(dupes_check))\n",
    "                    \n",
    "                # Write to CSV\n",
    "                temp_df.to_csv(f'newspaper_lineage_level_{str(lineage_value)}c.csv',index=False,header=True) \n",
    "            \n",
    "        # Overwrite dataframe that was originally fed into the for loop with the temporary dataframe contents. This will prompt the scraper to run through the newly compiled set of urls for the next lineage level. \n",
    "        df_loop_through = temp_df\n",
    "        print(lineage_value)\n",
    "                                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45129e67-47c5-491f-8eec-53bc9ff96c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-6261d2e502f3>:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_loop_through['current_lccn'] = df_loop_through['lccn']\n",
      "<ipython-input-17-6261d2e502f3>:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_loop_through['level'] = \"1\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# Run function\n",
    "family_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
